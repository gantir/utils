{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL with PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/projects/.pyenv/versions/3.7.10/envs/tatapower/lib/python3.7/site-packages/pyspark\"\n",
    "# os.environ[\"HADOOP_HOME\"] = \"\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "# os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/05 15:25:01 WARN Utils: Your hostname, Ramjees-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.7 instead (on interface en0)\n",
      "21/08/05 15:25:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/05 15:25:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/05 15:25:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession.\n",
    "spark = SparkSession.builder\\\n",
    "                .master(\"local[*]\")\\\n",
    "                .appName(\"ETL\")\\\n",
    "                .config(\"spark.executor.logs.rolling.time.interval\", \"daily\")\\\n",
    "                .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting filesystem and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all CSV's files from HiggsTwitter dataset (http://snap.stanford.edu/data/higgs-twitter.html)\n",
    "\n",
    "Read all the 5 different zip files into Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social Network edgelist\n",
    "\n",
    "# First, we set the filename\n",
    "file = \"HiggsTwitter/higgs-social_network.edgelist.gz\"\n",
    "\n",
    "# Second, Set the Schema where first column is follower and second is followed, both of types integer.\n",
    "schema = StructType([StructField(\"follower\", IntegerType()), StructField(\"followed\", IntegerType())])\n",
    "\n",
    "# Create the DataFrame\n",
    "socialDF = spark.read.csv(path=file, sep=\" \", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retweet Network\n",
    "\n",
    "# First, we set the filename\n",
    "file = \"HiggsTwitter/higgs-retweet_network.edgelist.gz\"\n",
    "\n",
    "# Second, Set the Schema where first column is tweeter, second is tweeted, third is occur and all are of type integer.\n",
    "schema = \n",
    "\n",
    "# Create the DataFrame\n",
    "retweetDF ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reply Network\n",
    "\n",
    "# First, we set the filename\n",
    "file = \"HiggsTwitter/higgs-reply_network.edgelist.gz\"\n",
    "\n",
    "# Second, Set the Schema where first column is replier, second is replied, third is occur and all are of type integer.\n",
    "schema = \n",
    "\n",
    "# Create the DataFrame\n",
    "replyDF = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention Network\n",
    "\n",
    "# First, we set the filename\n",
    "file = \"HiggsTwitter/higgs-mention_network.edgelist.gz\"\n",
    "\n",
    "# Second, Set the Schema where first column is mentioner, second is mentioned, third is occur and all are of type integer.\n",
    "schema = \n",
    "\n",
    "# Create the DataFrame\n",
    "mentionDF = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity Time\n",
    "\n",
    "# First, we set the filename\n",
    "file = \"HiggsTwitter/higgs-activity_time.txt.gz\"\n",
    "\n",
    "# Second, Set the Schema where \n",
    "#    * first column is userA (integer)\n",
    "#    * second is userB (integer)\n",
    "#    * third is timestamp (integer)\n",
    "#    * fourth is interaction (string): Interaction can be: RT (retweet), MT (mention) or RE (reply)\n",
    "schema = \n",
    "activityDF = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV's dataframes to Apache Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the five files to parquet format\n",
    "\n",
    "socialDF\n",
    "retweetDF\n",
    "replyDF\n",
    "mentionDF\n",
    "activityDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the parquet files into new dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the five files from parquet format\n",
    "\n",
    "socialDFpq = spark.read\n",
    "retweetDFpq = spark.read\n",
    "replyDFpq = spark.read\n",
    "mentionDFpq = spark.read\n",
    "activityDFpq = spark.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schema of the dataframes\n",
    "\n",
    "socialDFpq\n",
    "socialDFpq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 5 rows of each dataframe\n",
    "\n",
    "socialDFpq\n",
    "retweetDFpq\n",
    "replyDFpq\n",
    "mentionDFpq\n",
    "activityDFpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Users who have most followers\n",
    "\n",
    "socialDFpq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who have most mentions\n",
    "mentionDFpq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the top 5 followed users, how many mentions has each one?\n",
    "\n",
    "# top_f contains \"top 5 users who have most followers\"\n",
    "top_f = \n",
    "\n",
    "top_f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using SQL language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary views of all tables so we can use SQL statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views so we can use SQL statements\n",
    "socialDFpq.\n",
    "retweetDFpq.\n",
    "replyDFpq.\n",
    "mentionDFpq.\n",
    "activityDFpq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the tables in spark memory\n",
    "\n",
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who have most followers using SQL\n",
    "spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who have most mentions using SQL\n",
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Of the top 5 followed users, how many mentions has each one? (using SQL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GZIP Compressed CSV file vs Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GZIP Compressed CSV\n",
    "socialDF.groupBy(\"followed\").agg(count(\"follower\").alias(\"followers\")).orderBy(desc(\"followers\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parquet file\n",
    "socialDFpq.groupBy(\"followed\").agg(count(\"followed\").alias(\"followers\")).orderBy(desc(\"followers\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cached DF vs not cached DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will cache the 2 previous dataframes (socialDF and socialDFpq) and see how faster is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache dataframes\n",
    "socialDF.cache()\n",
    "socialDFpq.cache()\n",
    "\n",
    "# remove from cache\n",
    "#socialDF.unpersist()\n",
    "#socialDFpq.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: The first time we run cached dataframes can be slower, but the next times they should run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GZIP Compressed CSV (dataframe cached)\n",
    "socialDF.groupBy(\"followed\").agg(count(\"followed\").alias(\"followers\")).orderBy(desc(\"followers\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parquet file (dataframe cached)\n",
    "socialDFpq.groupBy(\"followed\").agg(count(\"followed\").alias(\"followers\")).orderBy(desc(\"followers\")).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
